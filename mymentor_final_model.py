# -*- coding: utf-8 -*-
"""MyMentor-FINAL MODEL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YhWI24wQFlU5d4NSp3buViPDF_DSvU0E
"""

!pip install gspread
!pip install pandas

from google.colab import drive, auth
from google.auth import default
import gspread as gd
import pandas as pd

drive.mount ('/content/drive')
auth.authenticate_user()

creds,_ = default()

gc = gd.authorize(creds)

data_mentor = gc.open('DATA_MYMENTOR').worksheet("Mentor")
data_mentee = gc.open('DATA_MYMENTOR').worksheet("Mentee")

data1 = data_mentor.get_all_values()
mentor = pd.DataFrame(data1)
mentor = mentor.shift(-1)
mentor.columns = ['Name','Latest_Education','Educational_Background','Experience','Work_Position','Role','Rating','Profile_Views']
mentor = mentor.iloc[:-1]
display(mentor)

data2 = data_mentee.get_all_values()
mentee = pd.DataFrame(data2)
mentee = mentee.shift(-1)
mentee.columns = ['Name','Latest_Education','Educational_Background','Role','Topic_1','Topic_2', 'Topic_3','Rating']
mentee = mentee.iloc[:-1]
display(mentee)

"""Preprocessing Data Mentor"""

dfdata_mentor = mentor[['Name','Work_Position','Role','Rating']]
dfdata_mentor

# List kata-kata tag
list_tag = ["Data Analyst", "Data Engineer", "Data Scientist", "Front end Developer", "Back end Developer", "Full Stack Developer", "Web Developer", "Software Engineer", "Information Security Analyst", "AI Specialist", "Computer Networking Specialist", "Database Administrator", "Business Development", "Technopreneur", "Business Consultant", "Risk Manager", "Product Manager", "Market Researcher", "Financial Analyst", "Digital Marketing", "Entrepreneur", "Digital Business Analysis", "Operational Manager", "Retail Manager", "Executive Business", "Graphic Designer", "UI/UX designer", "Product Developer", "Art Director", "Web Designer", "Art Entrepreneur", "Digital Strategist"]

# Fungsi untuk mencari kata dalam preferensi
def cari_kata(Preferensi):
    kata_ditemukan = []
    Preferensi_lower = Preferensi.lower()  # Mengubah preferensi menjadi huruf kecil

    for tag in list_tag:
        if tag.lower() in Preferensi_lower:
            kata_ditemukan.append(tag)

    return kata_ditemukan

# Membuat kolom tag
dfdata_mentor['tag'] = dfdata_mentor['Work_Position'].apply(cari_kata)

dfdata_mentor

from sklearn.preprocessing import MinMaxScaler

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# Mengambil nilai rating dari dataframe
ratings = dfdata_mentor['Rating'].values.reshape(-1, 1)

# Melakukan normalisasi fitur rating
normalized_ratings = scaler.fit_transform(ratings)

# Menyimpan hasil normalisasi ke dalam kolom "normalized_rating"
dfdata_mentor['Rating'] = normalized_ratings

# Tampilkan dataframe dengan fitur rating yang telah dinormalisasi
print(dfdata_mentor)

import nltk
nltk.download('stopwords')

nltk.download('punkt')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

class Mentee:
    def __init__(self, name, needs):
        self.name = name
        self.needs = needs

class Mentor:
    def __init__(self, name, expertise, rating):
        self.name = name
        self.expertise = expertise
        self.rating = rating

class MentoringPlatform:
    def __init__(self):
        self.mentees = []
        self.mentors = []
        self.vectorizer = TfidfVectorizer()

    def add_mentee(self, mentee):
        self.mentees.append(mentee)

    def add_mentor(self, mentor):
        self.mentors.append(mentor)

    def preprocess_text(self, text):
        stop_words = set(stopwords.words('english') + stopwords.words('indonesian'))
        tokens = word_tokenize(text.lower())
        filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]
        preprocessed_text = ' '.join(filtered_tokens)
        return preprocessed_text

    def fit_vectorizer(self):
        corpus = []
        for mentee in self.mentees:
            corpus.append(self.preprocess_text(mentee.needs))
        for mentor in self.mentors:
            corpus.append(self.preprocess_text(mentor.expertise))
        self.vectorizer.fit(corpus)

    def find_mentor(self, mentee):
        mentee_needs = self.preprocess_text(mentee.needs)
        mentee_vector = self.vectorizer.transform([mentee_needs])

        matched_mentors = []
        for mentor in self.mentors:
            mentor_expertise = self.preprocess_text(mentor.expertise)
            mentor_vector = self.vectorizer.transform([mentor_expertise])

            similarity_score = cosine_similarity(mentee_vector, mentor_vector)[0][0]
            if similarity_score > 0 :
                matched_mentors.append((mentor, similarity_score))
        matched_mentors = sorted(matched_mentors, key=lambda x: (x[1] + x[0].rating), reverse=True)

        return matched_mentors

# Contoh penggunaan:
platform = MentoringPlatform()

# Menambahkan mentees dan mentors ke platform
mentee1 = Mentee("Mentee 1", "Web Developer")
mentee2 = Mentee("Mentee 2", "Software Engineer")
mentee3 = Mentee("Mentee 3", "Data Analyst")
platform.add_mentee(mentee1)
platform.add_mentee(mentee2)
platform.add_mentee(mentee3)

for index, row in dfdata_mentor.iterrows():
    mentor_expertise = ' '.join(row['Work_Position'].split()[:10])  # Mengambil maksimal 10 kata dari mentor.expertise
    mentor = Mentor(row['Name'], mentor_expertise, row['Rating'])
    platform.add_mentor(mentor)

# Fitting vectorizer
platform.fit_vectorizer()

# Melakukan pencocokan mentee dengan mentor
for mentee in platform.mentees:
    matched_mentors = platform.find_mentor(mentee)
    if len(matched_mentors) > 0:
        print(f"Mentee '{mentee.name}' cocok dengan mentor:")
        for mentor, similarity_score in matched_mentors:
            print(f"- {mentor.name} ({mentor.expertise}) - Similarity Score: {similarity_score} - Rating: {mentor.rating}")
    else:
        print(f"Mentee '{mentee.name}' tidak ditemukan mentor yang cocok.")

matched_count = 0
total_mentees = len(platform.mentees)
for mentee in platform.mentees:
    matched_mentors = platform.find_mentor(mentee)
    if len(matched_mentors) > 0:
        matched_count += 1
accuracy = matched_count / total_mentees
print(accuracy)

k = 3  # Jumlah pilihan teratas yang ingin dievaluasi
matched_count = 0
total_mentees = len(platform.mentees)
for mentee in platform.mentees:
    matched_mentors = platform.find_mentor(mentee)
    top_k_mentors = matched_mentors[:k]
    if len(top_k_mentors) > 0:
        if mentee in [mentor[0] for mentor in top_k_mentors]:
            matched_count += 1
top_k_accuracy = matched_count / total_mentees
print(top_k_accuracy)

total_similarity_score = 0
total_matches = 0
for mentee in platform.mentees:
    matched_mentors = platform.find_mentor(mentee)
    if len(matched_mentors) > 0:
        total_matches += 1
        total_similarity_score += sum([mentor[1] for mentor in matched_mentors])
mean_similarity_score = total_similarity_score / total_matches
print(mean_similarity_score)

import networkx as nx
import matplotlib.pyplot as plt

# ... (previous code remains the same)

# Melakukan pencocokan mentee dengan mentor
graph = nx.Graph()
for mentee in platform.mentees:
    matched_mentors = platform.find_mentor(mentee)
    if len(matched_mentors) > 0:
        for mentor, similarity_score in matched_mentors:
            graph.add_edge(mentee.name, mentor.name)
    else:
        print(f"Mentee '{mentee.name}' tidak ditemukan mentor yang cocok.")

# Visualisasi grafik dengan gambar yang lebih besar dan garis yang lebih panjang
pos = nx.spring_layout(graph)
nx.draw_networkx_nodes(graph, pos, node_color='skyblue', node_size=100)
nx.draw_networkx_labels(graph, pos, font_size=6, font_family='sans-serif')
nx.draw_networkx_edges(graph, pos, width=0.5)  # Mengatur lebar garis menjadi 2.0
plt.figure(figsize=(15, 10))  # Menentukan ukuran gambar
plt.axis('off')
plt.show()

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense, Dropout

# Membaca data dari file atau sumber data lainnya
data = dfdata_mentor  # Lakukan langkah pengambilan data sesuai dengan sumber data Anda

# Memisahkan fitur dan label
X = data['Name']  # Fitur place_name
y = data['Work_Position']  # Label kategori

# Melakukan encoding kategori menjadi bilangan bulat
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Melakukan one-hot encoding pada data kategori
num_classes = len(label_encoder.classes_)
y = tf.keras.utils.to_categorical(y, num_classes)

# Melakukan pengkodean fitur dengan LabelEncoder atau Encoder lainnya
feature_encoder = LabelEncoder()
X_encoded = feature_encoder.fit_transform(X)

# Normalisasi fitur menggunakan StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded.reshape(-1, 1))

# Membagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Membangun model content-based filtering
model = Sequential()
model.add(Dense(256, activation='relu', input_dim=1))
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))

# Mengompilasi model
#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.01),
    metrics=['accuracy']
)
# Melatih model dengan data latih
#history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))

history = model.fit(
    x=X_train,
    y=y_train,
    batch_size=20,
    epochs=1000,
    validation_data=(X_test, y_test),

)

# Evaluasi model dengan data uji
loss, accuracy = model.evaluate(X_test, y_test)

print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()

# Define parameter grid
param_vals = {'max_depth': [200, 500, 800, 1100],
              'n_estimators': [100, 200, 300, 400]}

# Perform Randomized Search
random_rf = RandomizedSearchCV(estimator=model, param_distributions=param_vals,
                              n_iter=10, scoring='accuracy', cv=5,
                              refit=True, n_jobs=-1)

# Training
random_rf.fit(X_train, y_train)

# Prediction
preds = random_rf.best_estimator_.predict(X_test)

# Print best parameters and accuracy
print('Best Parameters:', random_rf.best_params_)
print('Accuracy:', random_rf.best_score_)

from sklearn.metrics import accuracy_score
model = RandomForestClassifier(n_estimators=100, max_depth=200)

history = model.fit(
    X_train,
    y_train
)
preds = model.predict(X_test)
accuracy = accuracy_score(y_test,  preds)
print("Accuracy:", accuracy)

"""Save model Convert to TFLITE atau JSON"""

from sklearn.ensemble import RandomForestClassifier
from joblib import dump

# Membuat dan melatih model Random Forest Classifier
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Menyimpan model ke file
dump(model, 'my_model')